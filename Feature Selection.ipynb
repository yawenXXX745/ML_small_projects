{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1904120c-6736-4336-a933-142620e00117",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "### Hughes Phenomenon\n",
    "* Hughes phenomenon shows that as the number of features increases, the classifier's performance increases as well until we reach the optimal number of features.\n",
    "* Adding more features based on the same size as the training set will then degrade the classifier's performance.\n",
    "\n",
    "### Filter Methods\n",
    "* In order to reduce numeric features, Correlation Coefficients can be used to find the correlation between the various independent variables and the dependent variables.\n",
    "* In order to reduce categorical features, chi-square can be used.\n",
    "\n",
    "### Wrapper Methods\n",
    "* Wrapper methods is an automatic feature reduction process which involves no human intervention\n",
    "* We build a machine learning model and it picks features in each step of the model building process thereby selecting features until it reaches the highest accuracy.\n",
    "* Various predictive models are used to score feature subsets based on the coefficients, error rate of the model and other statistical metrics such as t-stats, R-square, AIC, etc.\n",
    "\n",
    "    Original Set of Features --> Generation of feature subsets --> Evaluation of feature subsets -->(goodness of model) stopping criterion -->(if good enough) validation of model                                                                                                             -->(if not good enough) go back to generation of feature subset.\n",
    "\n",
    "### Embedded Methods\n",
    "* First, we need to have the features on the same scale.\n",
    "* Then we can simply presume that the features that have the highest coefficients in the model are the ones that are important.\n",
    "* The features that are not correlated to the output variable have coefficient values close to zero or even zero.\n",
    "* Under the embedded method, different regularization methods are used, the most common methods are Ridge regression and Lasso regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae01788e-e6a7-4847-b37c-37c37600d9df",
   "metadata": {},
   "source": [
    "## Train and Test Split\n",
    "* We should generate features separately on train and test datasets.\n",
    "* If want to normalize the data, we should use the mean of train, because if we use the mean of all data, it will cause the leaking information of test set. Therefore, only use the mean and variance of training set to normalize the training set. When at the test stage, use the same mean and variance in training process to normalize the test set.\n",
    "* A standard (but often implicit) assumption is that the training & test sets are qualitatively similar.\n",
    "* The test set is there solely for performance assessment of your model, and it should not be used in any stage of model building, including feature selection.\n",
    "* During all stages of model building (including feature selection), pretend that you don't have access to the test set at all, and it will becomes available only when you need to assess the performance of your final model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
