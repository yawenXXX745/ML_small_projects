{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92c3bb53-2d5c-4b7f-bea3-fcd731731528",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2382318-a2bd-4c59-bc1d-ba17f9418183",
   "metadata": {},
   "source": [
    "### Four Levels of Data Analysis\n",
    "\n",
    "* Decriptive Analytics\n",
    "* Diagnostic Analytics\n",
    "* Predictive Analytics\n",
    "* Prescriptive Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892d2ae7-5b4f-4dc9-a1fb-30a6be3472d0",
   "metadata": {},
   "source": [
    "### CRISP-DM process for modeling\n",
    "CRISP-DM stands for Cross Industry Standard Process for Data Mining. The word data mining can be interchanged with Predictive Analytics.\n",
    "\n",
    "#### Crisp-DM process steps:\n",
    "1) Start with business understanding of what you want to do with data mining.\n",
    "2) Data Understanding & Interactions between business understanding and data understanding.\n",
    "3) Data Preparation & Interactions between data preparation and modeling.\n",
    "4) Modeling and Assessment(Evaluation) & Interactions between model evaluation and business understanding.\n",
    "5) Deployment of model\n",
    "6) Results achieved from PA should be compared with the business understanding.\n",
    "\n",
    "#### Crisp-DM step 4:\n",
    "* Select model techinques: Modeling techniques & Modeling assumption\n",
    "* Generate test design: Test Design\n",
    "* Build Model: Parameter settings & Models & Model description\n",
    "* Assess model: Model assessment & Revised parameter settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5ded8e-e2c4-4703-be78-71a071f3c970",
   "metadata": {},
   "source": [
    "### Supervised & Unsupervised Learning\n",
    "\n",
    "* Supervised Learning: Classification, Regression \n",
    "  ##### Classification. \n",
    "  y is discrete, such as spam email or not. If there are only two classes, it is binary classifier. \\\n",
    "  A decision boundary is a line (in the case of two features), where all (or most) samples of one class are on one side of the line, and all samples of the other class are on the opposite side of that line. The line separates one class from the other. \\\n",
    "  If you have more than two features, the decision boundary is not a line, but a hyper-plane in the dimension of your feature space.\\\n",
    "\n",
    "  ##### Regression.\n",
    "  y is continuous, such as income, weight of fruit, etc.  \n",
    "  \n",
    "* Unsupervised Learning: Clustering\n",
    "\n",
    "* Semi-supervised Learning\n",
    "  Combines labeled and unlabeled data during training to improve performance:\n",
    "  ##### Semi-supervised classification\n",
    "  Training on labeled data exploits additional unlabeled data, frequently resulting in a more accurate classifier.\n",
    "  ##### Semi-unsupervised clustering\n",
    "  Uses small amount of labeled data to aid and bias the clustering of unlabeled data.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b31d882-9126-4c1b-814a-68b3e24a9c9e",
   "metadata": {},
   "source": [
    "### Training and Testing\n",
    "\n",
    "How can we confident about the model f we trained? We calculate the Etrain the in-sample error (training error or empirical error/risk) \\\n",
    "\n",
    "Etrain = (sigma i from 1 to N) loss(yi, f(xi))\\\n",
    "\n",
    "##### Examples of Loss Function\n",
    "    * Classification error:\n",
    "    loss(yi, f(xi)) = 0 if yi = f(xi)\n",
    "                    `1 if yi != f(xi)\n",
    "    * Least Square Loss:\n",
    "    loss(yi, f(xi)) = (yi - f(xi))^2\n",
    "\n",
    "##### We aim to minimize the Etrain error(loss). \n",
    "We also want the out-sample error Etest to be small too. \n",
    "\n",
    "##### Underfitting & Overfitting\n",
    "The bias could be high while variance is low when the model is underfitting, however, the bias is low while variance is high when model is overfitting. \n",
    "\n",
    "##### How to avoid overfitting?\n",
    "* Reduce the number of features manually or do feature selection\n",
    "* Do a model selection\n",
    "* Use regularization (keep the features but reduce their importance by setting small parameter values)\n",
    "* Do a cross-validation to estimate the test error.\n",
    "\n",
    "##### Regularization\n",
    "* L = sigma((y - f(x))^2) + alpha*sigma(m^2)\n",
    "  where, sigma((y - f(x))^2) is the sum of squared residuals\n",
    "          alpha * sigma(m^2) is the penalty, m is related to model complexity, the more complex the model, the higher the penalty.\n",
    "\n",
    "#### Train, Validation and Test\n",
    "The dataset is divided into training set, validation set, and testing set. \\\n",
    "The training set are used to train multiple models, such as KNN, logistic Regression, etc. \\\n",
    "The validation set are used to validate models, including tuning hyper parameters and select the best model. \n",
    "##### The training set together with the validation set are used in the training process.\n",
    "The testing set are used to evaluate model based on various metrics, such as confusion matrix to evaluate the final performance of the selected model. \n",
    "\n",
    "##### Cross Validation\n",
    "* In the basic approach, the training set is split into k smaller sets, is called k-fold cross validation.\n",
    "* The following procedure is followed for each of the k 'folds':\n",
    "  * A model is trained using k-1 of the folds as training data\n",
    "  * The resulting model is validated on the remaining one fold of the data(i.e. it is used as test set to compute a performance measure such as accuracy)\n",
    "  * The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9189af-d3fc-4bbf-9d06-ad28a14c2b2b",
   "metadata": {},
   "source": [
    "### Modeling Methods\n",
    "\n",
    "1) Check if needs dimension reduction: PCA, SVD, LDA\n",
    "2) Check if data have responses(labeled): Hierarchical, DBSCAN, K-modes, K-means, GMM\n",
    "3) Predictin Continuous variable or Discrete variable?\n",
    "   * Numeric: Neural network, Ramdom forest, Gradient boosting tree, Decision tree, Linear regression\n",
    "   * Classification: Kernel SVM, Neural network, Gradient boosting tree, Random forest, Decision Tree, Logistic Regression, Naive Bayes, Linear SVM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
